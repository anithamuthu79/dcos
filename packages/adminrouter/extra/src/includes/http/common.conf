# Enable VTS module for metrics.
#
# The VTS module uses shared memory to store metrics data.
# In particular, we store metrics data generated by the
# ``vhost_traffic_status_filter_by_set_key`` directive.
#
# If this memory is used, Admin Router does not crash, but errors are shown
# in the log, and metrics are no longer collected.
# The amount of memory allocated here is allocated on each DC/OS node.
# The number of metrics data items (called nodes) stored on each DC/OS node
# scales with the number of DC/OS nodes in the cluster.
#
# Allocating more memory:
# - (+) More metrics nodes means more data available to use.
# - (+) If we hit the max node count, we will remove old data - this only is an
# issue if this data has not been scraped, so the more memory we have, the more
# flexible we can be with the scraping interval
# - (-) Increased memory usage and requirements for DC/OS
# - (-) Slow metrics page = slow to scrape
#
# The shared memory zone size must be sufficiently large to ensure that all
# metrics that could potentially be generated by the VTS module's
# vhost_traffic_status_filter_by_set_key directive can be stored. If the
# shared memory zone limit is hit Admin Router will not create any new metric
# nodes. The metrics data
# scales with the number of metric nodes. A metric node in our case is a set
# of data about one detail, such as the number of requests made with a
# particular client, or the number of responses with a particular status code,
# or the number of URIs requested from Admin Router. In theory, the number of
# URIs can be infinitely large. Therefore, we risk running out of memory if
# the number of URIs requested grows large.
#
# # TODO: (E2E) test that AR survives full shared memory
#
# Set shared memory size to 16MB. This accommodates ~4000 metric nodes. Reserve
# 1300 metric nodes for Admin Router agent forwarding where each agent is
# counted as distinct upstream in the order of at maximum 1000 agents.
# Upstreams are seemingly not accounted for by the VTS module LRU mechanism so
# we must always keep enough room for them.
vhost_traffic_status_zone shared:vhost_traffic_status:16m;
#
# The metrics data scales with the number of metric nodes. A metric node in our
# case is a set of data about one detail, such as the number of requests made
# with a particular client, or the number of responses with a particular status
# code, or the number of URIs requested from Admin Router. In theory, the
# number of URIs can be infinitely large. Therefore, we risk running out of
# memory if the number of URIs requested grows large, unless we limit the
# number of metrics nodes.
#
# Set max allowed metric nodes before LRU mechanism kicks in to 2700.
vhost_traffic_status_filter_max_node 2700
# This implies in the worst-case metrics for 2700 / 3 (for status, URI, client)
# = 900 requests can be safely stored at any given time. Given a Telegraf
# scrape interval of 10 seconds this means 90 requests/s is a safe rate at
# which we expect no metrics to be missed despite the worst-case (status, URI,
# client) tuple distribution. In MWT #11 we did not see 100 requests/s being
# reached across all Master Admin Router instances so this should suffice.
#
# The aforementioned numbers were evaluated in the following experiment:
#
# 1. Call random 128 characters long random URLs on Admin Router in a loop.
# 2. Wait for the shared memory to fill up until the LRU mechanism activates.
# This resulted in in 2701 nodes being used -> 9.2MB additional metric data.
#
# 3. Node internal measurements (Telegraf perspective):
# - curl http://localhost/nginx/metric (Downloading 10.6MB of data takes < 1
# second, nearly instant even.)
#
# 4. Node external measurements (Telegraf endpoint):
# - curl http://172.17.0.2:61091/metrics takes < 3 seconds on Docker
# (Downloading 53MB of metric data on an otherwise empty cluster)
#
# Going forward we will implement a dedicated Admin Router Telegraf plugin
# which vastly reduces the number of reported useless metrics.
# From then on, only the node internal scraping time for Nginx metrics is
# impacted by the settings above, which eases decision making on the
# Security Team side. Until then, this the settings above are believed to
# present a reasonable trade-off to accommodate metrics for internal
# testing.

client_max_body_size 1024M;

# Define custom log format. Use the default 'combined'
# format and append easily parsable timing information
# for performance analysis, and other relevant debugging
# information.
log_format customformat '$remote_addr - $remote_user [$time_local] '
    '"$request" $status $body_bytes_sent '
    '"$http_referer" "$http_user_agent" '
    'logtime_msec=$msec '
    'upstream_addr=$upstream_addr '
    'request_time=$request_time '
    'upstream_response_time=$upstream_response_time '
    'upstream_connect_time=$upstream_connect_time '
    'upstream_header_time=$upstream_header_time';

# The syslog facility here is set to daemon because
# systemd SyslogFacility defaults to daemon and
# therefore all other DC/OS services log to it.
# https://jira.mesosphere.com/browse/DCOS-38622
access_log syslog:server=unix:/dev/log,facility=daemon customformat;
include mime.types;
default_type application/octet-stream;
sendfile on;
keepalive_timeout 65;

server_tokens off;

lua_package_path '$prefix/conf/lib/?.lua;;';


# Name: DC/OS Component Package Manager (Pkgpanda)
# Reference: https://dcos.io/docs/1.10/administering-clusters/component-management/
upstream pkgpanda {
    server unix:/run/dcos/pkgpanda-api.sock;
}

# Name: DC/OS Log
# Reference: https://dcos.io/docs/1.10/monitoring/logging/logging-api/
upstream log {
    server unix:/run/dcos/dcos-log.sock;
}

# Name: DC/OS Checks API
upstream dcos_checks_api {
    server unix:/run/dcos/dcos-checks-api.sock;
}
